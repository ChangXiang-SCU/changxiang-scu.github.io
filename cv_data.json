{
  "raw_text": "Xiang Chang \n1 E Loop Rd, New York, NY 10044 | 917-497-3270 | xc529@cornell.edu \nEDUCATION \n \nCornell Tech (Cornell University)\u2013New York, NY Aug. 2024\u2013May. 2026 \nJacobs Technion-Cornell Dual Master of Science Degrees \u2013 Connective Media Concentration 4.0/4.3 \n \nSichuan University (SCU)\u2013Chengdu, China Sept. 2019\u2013July. 2023 \nBachelor of Management in Information Resource Management 3.75/4.0 \n \nPUBLICATIONS \n \n[1] Xiang Chang, Zhijie Yi, Yichang Liu, Hongling Sheng, Dengbo He. 2025. The Formation of Trust \nin Autonomous Vehicles after Interacting with Robotaxis on Public Roads. (ASPIRE 2025). \nhttps://doi.org/10.1177/10711813251358236 \n \n[2] Chishang Yang, Xiang Chang, Debargha Dey, Zhuoqi Xu, Avi Parush, and Wendy Ju. 2025. \nSocially Adaptive Autonomous Vehicles: Effects of Contingent Driving Behavior on Drivers' \nExperiences (AutomotiveUI 2025). https://doi.org/10.1145/3744333.3747814 \n \n[3] Zhenyu Wang, Haolong Hu, Weiyin Xie, Xiang Chang, Peixuan Xiong, and Dengbo He. 2025. \nExploring User Needs in Fully Driverless Robotaxis: A Think-Aloud Study of First-Time On-Road \nRides (AutomotiveUI 2025 Works in Progress).https://doi.org/10.1145/3744335.3758506 \n \n[4] Xiang Chang, Zihe Chen, Xiaoyan Dong, Yuxin Cai, Tingmin Yan, Haolin Cai, Zherui Zhou, \nGuyue Zhou, and Jiangtao Gong. 2024. \"It Must Be Gesturing Towards Me\": Gesture-Based \nInteraction between Autonomous Vehicles and Pedestrians. (CHI 2024). \nhttps://doi.org/10.1145/3613904.3642029 \n \n[5] Xinru Tang, Xiang Chang, Nuoran Chen, Yingjie Ni, RAY LC, and Xin Tong. 2023. Community-\nDriven Information Accessibility: Online Sign Language Content Creation within d/Deaf \nCommunities. (CHI 2023). https://doi.org/10.1145/3544548.3581286  \n \n[6] Jin, Rongxin; Liu, Yifan; Yang, Lichen; and Chang, Xiang, \"Influencing factors of resident \nsatisfaction in smart community services: An empirical study in Chengdu\". (ICEB 2022). \nhttps://aisel.aisnet.org/iceb2022/25 \n \n[7] Li, Chenyu; Sun, Chengxi; Chang, Xiang; Liang, Luoming; Ma, Yao; and Ke, Fan, \"Study on the \nInfluencing Factors of Health Information Sharing Behavior of the Elderly under the Background of \nNormalization of Pandemic Situation\". (ICEB 2021 Best Paper).  https://aisel.aisnet.org/iceb2021/48 \n \nWORKING MANUSCRIPTS \n \n[1] Jiangtao Gong, Yueteng Yu, Yancheng Cao, Ruoxuan Yang, Xiang Chang, Haoming Tang, Xiaoji \nZheng, Yiyao Liu, Shanhe You, Chen Zheng, and Guyue Zhou. 2025. An EEG Dataset for \nUnderstanding Driving Expertise from Naturalistic Urban Road Experiments. (Scientific Data Under \nReview). https://github.com/AIR-DISCOVER/ExpertDrivingDataset.git \n \n[2] Zhijie Yi*, Xiang Chang*, Yueteng Yu, Xinyu Yang, Junrong Lu, Yiyao Liu, Ye Jin, Mengdi Chu, \nJingli Qin, Jialin Song, and Jiangtao Gong. 2025. From Driver to Passenger: Understanding \nEvaluation Gaps in 'Fantastic' Driving Behaviour Delivery. (CSCW 2026 Under Review). \n \n[3] Yuanchen Bai, Zijian Ding, Shaoyue Wen, Xiang Chang, and Angelique Taylor. 2025. From MAS \nto MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic \nSystems within a Healthcare Scenario. (AAAI 2026 Under Review). \nhttps://arxiv.org/abs/2508.04691 \n[4] Zhenyu Wang, Weiyin Xie, Haolong Hu, Xiang Chang, Meng Sun, and Dengbo He. 2025. Users\u2019 \nTrust Evolvement in Fully Driverless Robotaxis During First Ride: An On-road Study. (Human \nFactors Under Review). \nRESEARCH EXPERIENCE \n \nVisiting Student Researcher, Stanford University                                         Jun. 2025\u2013Aug. 2025 \nMind-Wandering Detection in Automated Urban Driving  \nAdvisor: Dr. Rebecca Currano & Mark Cutkosky  \n\u2022 Designed a follow-on VR automated-driving experiment contrasting urban and lower-complexity road \ncontexts to probe mind-wandering under reduced control demands. \n\u2022 Built a real-time data pipeline that integrates Unity with eye tracking (gaze and pupil) and PPG, \nincluding automatic event tagging and synchronized timestamps using Tobii XR SDK, Polar SDK. \n\u2022 Implemented IBI and HRV feature extraction (for example, RMSSD and SDNN) with artifact \nhandling in NeuroKit2, producing analysis-ready logs.         (In Progress) \n \nGraduate Researcher, Cornell University                               Aug. 2024\u2013Present \nMulti-Agent Robotic Systems for ER Onboarding (Healthcare MARS)  \nAdvisor: Professor Angelique Taylor \n\u2022 Co-developed a hierarchical evaluation framework: robot roles, tools, and dependency-aware tasks for \nhigh-risk, low-tolerance medical workflows. \n\u2022 Curated a 5-part knowledge base and 7 behavioral metrics, running controlled comparisons on \nCrewAI; improved average success from 45.29% to 72.94% and distilled five key failure modes to \nguide redesign. \n\u2022 Implemented agents bidirectional feedback in AutoGen (real-time oversight + agent self-reflection), \nreaching 88.97% success and stronger fault handling. \n\u2022 Benchmarked o3 vs. GPT-4o, mapping behaviors across planning granularity, role comprehension, \nformat compliance, and termination validation; concluded that reasoning boosts autonomy but needs \nstructural constraints for stability.             (Submitted to AAAI 2026) \n \nUnderstanding Human-Driver Interaction and Negotiation with Autonomous Vehicles  \nAdvisor: Professor Wendy Ju & Debargha Dey  \n\u2022 VR study with four intersection scenarios comparing a contingent AV trained on human interaction \ndata to two non-contingent baselines that always yield or never yield. Measures included hesitance, \nrelaxation, stress, and perceived familiarity. \n\u2022 The contingent policy was rated most familiar and produced lower stress and higher relaxation. \nAlways-yield increased hesitance; non-yield signaled intent more clearly but could elevate stress \ndepending on context. \n\u2022 Favor context-adaptive, human-like AV policies; tune conservativeness to traffic conditions; pair \nbehavior with clear, transparent negotiation cues to support driver comfort and trust.  \n(Accepted by AutomotiveUI 2025) \n \nTransforming Robotic Cart Interactions: How Voice Commands and Drawer Lights Improve Clinical \nTeamwork \nAdvisor: Professor Angelique Taylor \n\u2022 Developed a Wizard-of-Oz controlled Robotic Crash Cart (RCC) prototype to assist healthcare \nworkers (HCWs) in emergency rooms using multimodal feedback, including voice commands and \ndrawer lights, for supply location and medication reminders during critical medical tasks. \n\u2022 Conducted in-lab studies with 84 participants to assess the effectiveness of RCC feedback in reducing \nworkload, improving task performance, and enhancing teamwork in high-pressure environments. \n(In Progress) \n \nResearch Assistant, Hong Kong University of Science and Technology      Oct. 2023\u2013Jan 2025 \nAdvisor: Professor Dengbo He (HKUST) \nMultimodal Dataset for Pedestrian-Autonomous Vehicle Interaction Based on On-Road Experiment \n\u2022 Conducted experiments involving pedestrian-autonomous vehicle interaction, using multimodal data \ncollected from on-road (real-world) environment. \n\u2022 Equipped participants with EEG, ECG, eye-tracking devices, and other physiological sensors to \nmonitor their reactions and using a drone to capture precise walking and driving trajectories. \n\u2022 Designed and administered pre-experiment and post-experiment questionnaires, assessing pedestrian \nbehavior, receptivity, Personal Innovativeness, pre and post-interaction Trust after repeated crossings \nwith robotaxi.                                         (In Progress/ Accepted by ASPIRE2025) \n \nPassengers\u2019 Trust Evolvement in Fully Driverless Robotaxis \n\u2022 Ran real-world first-ride sessions on fully driverless L4 robotaxis in Guangzhou using a fixed urban \nroute (15 km, 40 min) with N=30 participants and bi-minute trust ratings + concurrent think-aloud. \n\u2022 Modeled dynamic trust with a cumulative-link mixed model: trust rose then stabilized; individual \ndifferences (e.g., prior knowledge, personality) moderated trajectories. \n\u2022 Thematically analyzed in-situ verbal reports; distilled core user needs: perceived safety, efficiency, \ncomfort; conservative/human-like driving and transparent HMI supported trust, while expectation \nmismatches reduced it. \n\u2022 Delivered design implications: context-adaptive driving styles, explanatory/transparent HMI, and \ntunable conservativeness to balance safety, efficiency and comfort.  \n        (Submitted to Human Factors/ Accepted by AutoUI2025) \n \nResearch Assistant, Tsinghua University Mar. 2023\u2013Sept. 2023 \nAdvisor: Professor Jiangtao Gong (THU AIR) \nVirtual Hand: Interaction Between Pedestrians and Autonomous Vehicles \n\u2022 Through interviews with drivers and pedestrians, understand their commonly used interaction \nmethods. Design a new eHMI. Generate a virtual hand in the front window of the car to convey the \nintention of autonomous driving through different gestures. Eight common gestures were selected to \nconvey AVs' yielding or non-yielding intentions at uncontrolled crosswalks. \n\u2022 Design and complete experiments, collect experimental data (Duration of observation, Error Rate, \nPerception of danger, Physiological indicators, etc.), analyze and compare to confirm the best eHMI. \n\u2022 Through a VR experiment (N1 = 31) and an online survey (N2 = 394), we discovered significant \ndifferences in the usability of gesture-based eHMIs compared to traditional eHMIs. Better gesture-\nbased eHMIs increase the efficiency of pedestrian-AV interaction while ensuring safety. Poor \ngestures, however, cause misinterpretation and, consequently, accidents. (Accepted by CHI2024) \n \nAn Electroencephalography (EEG) Dataset for Expert-like Autonomous Driving through Actual \nDriving Tests in Real Urban Road Context \n\u2022 Using the EEG, eye tracking, physiological behavior information of expert drivers, as well as the \nsubjective evaluation of passengers, as indicator basis, provide research datasets in the neuroscience \nand interaction fields of expert driving. \n\u2022 Design experiments and have completed 20 sets: one expert driver drives the vehicle through urban \nroad routes, and two passengers ride in the vehicle to assist in evaluating the driver's driving behavior. \n\u2022 Collect driver's EEG, eye tracking, heart rate, skin electroencephalography, driving behavior, video \nrecording of passenger posture inside the car, CAN-bus data, panoramic video recording of the outside \nenvironment, etc. Analyze and compare the differences between expert drivers and regular drivers. \n\u2022 Analyze the subjective questionnaires of drivers and passengers, and conduct in-depth interviews with \nthem after the experiment. From the interview content, the evaluation criteria for driver driving \nbehavior are obtained through coding.                               (Submitted to Scientific Data & CSCW2026) \n \nSummer Research Intern, HCI\u00b7 X May. 2022\u2013Jun. 2023 \nAdvisor: Professor Ray LC (CityU) & Professor Xin Tong (Duke Kunshan University)  \nCommunity-driven information accessibility: Online sign language content creation within d/Deaf \ncommunities \n\u2022 Compared sign languages used in China and the United States and analyzed unique challenges faced \nby d/Deaf communities in China. \n\u2022 Consulted teachers in schools for the deaf and social workers about dos and don\u2019ts when interviewing \nd/Deaf people, developed an interview guide, and designed interview questions. \n\u2022 Connected with 60+ d/Deaf people in China and recruited 12 interview participants, conducted semi- \nstructured interviews with all participants for a total of 30+ hours, coded 300+ videos, wrote a 18,000- \nword interview transcript, and analyzed the participants\u2019 responses to understand d/Deaf people\u2019s \naccess to online sign language content and how they were involved in content creation. \n\u2022 Found that d/Deaf communities can support the growth of information in sign language; identified the \nways how d/Deaf people collaborate and negotiate information accessibility together online, as well as \nthe challenges in community-driven accessibility for d/Deaf people.                    (Accepted by CHI2023) \n \nUndergraduate Researcher, Sichuan University Feb. 2021\u2013Oct. 2021 \nAdvisor: Professor Ying Zhao, SCU \nStudy on Health Information Sharing Behavior of the Elderly During COVID-19 \n\u2022 Conducted a questionnaire survey in neighboring communities to collect information behavior data of \nelderly citizens (500 questionnaires distributed and 472 valid ones collected) \n\u2022 Built a health information sharing model for the elderly based on the MOA model and autonomy \ntheory and analyzed it with Amos structural equation model. \n\u2022 Found that media richness, health information literacy, perceived benefits and negative emotions of \nthe epidemic situation positively impact health information sharing behavior, while perceived risks \nhave a significant negative impact on health information sharing behavior.     (Accepted by ICEB2022) \n \nStudy on the influence of smart community construction on citizens\u2019 level of satisfaction with public \nservices\u2014A case study in Chengdu  \nProject Leader | Provincial Project, China College Students Innovation and Entrepreneurship Competition \n\u2022 Conducted literature review, applied expectation disconfirmation theory and self-regulation theory \nwith the American Customer Satisfaction Index (ACSI) to construct a model to describe how smart \ncommunity construction influences citizens\u2019 level of satisfaction with public services. \n\u2022 Conducted questionnaire surveys in three communities with different levels of smart infrastructure \nconstruction (372 questionnaires distributed and 342 valid ones collected). Analyzed the questionnaire \ndata and ran hypothesis testing using structural equation model \n\u2022 Conducted semi-structured interviews with officials in charge of the smart transformation projects, \nutilized the third level of coding in grounded theory to perform text analysis in order to explore the \nsystem issues with smart community construction.              (Accepted by ICEB 2021) \n \nINTERNSHIP \n \nResearch Assistant, Hong Kong University of Science and Technology                Oct. 2023\u2013July. 2024 \n \nIntern Researcher, Institute for AI Industry Research, Tsinghua University      Mar. 2023\u2013Sep. 2023 \n \nResearch Assistant, OPPO research institute Dec. 2022\u2013Mar. 2023 \n \nProduct Intern, Chengdu Business Big Data Co., Ltd. Apr. 2021\u2013July. 2021 \n \nSKILLS \n \nProgramming Languages: Python, C++, SQL, PHP, C#, HTML/CSS/JavaScript \nXR Development: Unity, Unreal Engine, Tobii XR SDK \nSensors & Signal Processing: Tobii Pro Lab, D-Lab, EEG/PPG/EDA pipeline, NeuroKit2, Polar sensor \nSDK, CAN-bus \nExperimental Tools: Lab Streaming Layer, MQTT, ATLAS.ti, E-Prime, HRT \nStatistics: SPSS, Stata, SAS, R \nHardware / Robotics / Simulators: ROS, Raspberry Pi, CircuitPython, Baidu Apollo, CARLA \nMulti-Agent Frameworks: AutoGen, CrewAI \n \nACADEMIC SERVICES \n \nPeer Review Experience: CHI 2023/2024/2025/2026, Transportation Research Part F, AutoUI2025 \nSession Chair: ASPIRE 2025 (ST4: Pedestrian Behavior & Interactions) \n",
  "links": [
    "mailto:xc529@cornell.edu",
    "https://doi.org/10.1177/10711813251358236",
    "https://doi.org/10.1145/3744333.3747814",
    "https://doi.org/10.1145/3744335.3758506",
    "https://doi.org/10.1145/3613904.3642029",
    "https://doi.org/10.1145/3544548.3581286",
    "https://aisel.aisnet.org/iceb2022/25",
    "https://aisel.aisnet.org/iceb2021/48",
    "https://github.com/AIR-DISCOVER/ExpertDrivingDataset.git",
    "https://arxiv.org/abs/2508.04691"
  ]
}